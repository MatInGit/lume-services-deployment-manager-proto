{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json\n",
    "\n",
    "json_data = json.load(open(\"cred.json\"))\n",
    "# print(json_data)\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = json_data[\"MLFLOW_TRACKING_USERNAME\"]\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = json_data[\"MLFLOW_TRACKING_PASSWORD\"]\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = json_data[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"]\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = json_data[\"AWS_DEFAULT_REGION\"]\n",
    "os.environ[\"AWS_REGION\"] = json_data[\"AWS_REGION\"]\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = json_data[\"AWS_ACCESS_KEY_ID\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = json_data[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = json_data[\"MLFLOW_S3_ENDPOINT_URL\"]\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = json_data[\"MLFLOW_TRACKING_URI\"]\n",
    "\n",
    "\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "from mlflow.server import get_app_client\n",
    "import mlflow.pytorch\n",
    "import mlflow.keras\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.set_experiment(\"my-experiment\")\n",
    "\n",
    "data_X = np.random.uniform(-1, 1, (1000, 2))\n",
    "data_y = np.max(data_X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model_definition import ExampleModel\n",
    "from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n",
    "\n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model_name, model):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "\n",
    "    # this function is called when the model is loaded using pyfunc.load_model\n",
    "    def predict(self, context, input, **kwargs):\n",
    "        return self.model.evaluate(input)\n",
    "    \n",
    "    def inverse_predict_internal(self, input, **kwargs):\n",
    "        return np.sqrt(input)\n",
    "\n",
    "    def save_model(self):\n",
    "        with open(f\"{self.model_name}.txt\", \"w\") as f:\n",
    "            f.write(\"model saved\")\n",
    "\n",
    "    def load_model(self):\n",
    "        with open(f\"{self.model_name}.txt\", \"r\") as f:\n",
    "            return f.read()\n",
    "        \n",
    "    def get_lume_model(self):\n",
    "        return self.model\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "input_variables = [ScalarInputVariable(name = \"x1\", default=0, value_range=[-100000, 1000000]), ScalarInputVariable(name = \"x2\", default=0, value_range=[-100000, 1000000])]\n",
    "output_variables = [ScalarOutputVariable(name = \"y\")]\n",
    "lume_model = ExampleModel(input_variables = input_variables, output_variables = output_variables)\n",
    "model = MyModel(\"model1\", lume_model)\n",
    "\n",
    "input_sample = pd.DataFrame(data_X, columns=[\"x1\", \"x2\"])\n",
    "\n",
    "with mlflow.start_run() as run:  # you can use run_name=\"test1\" to give a name to the run otherwise it will a random name\n",
    "\n",
    "    \n",
    "    # set some tags for the experiment\n",
    "    mlflow.set_tag(\"exp_tag1\", \"exp_tag_value1\")\n",
    "    mlflow.set_tag(\"exp_tag2\", \"exp_tag_value2\")\n",
    "    mlflow.set_tag(\"exp_tag3\", \"exp_tag_value3\")\n",
    "    \n",
    "    # model.save_model() # no need to save the model since it is saved in log_model\n",
    "    mlflow.log_param(\"model_name\", model.model_name)\n",
    "    mlflow.log_param(\"dummy_param1\", \"dummy_value1\")\n",
    "    mlflow.log_param(\"dummy_param2\", 0.33)\n",
    "    for i in range(10):        \n",
    "        mlflow.log_metric(\"metric1\", (i / 10) ** 2 , step=i)\n",
    "        mlflow.log_metric(\"metric2\", (i / 10) ** 3 , step=i)\n",
    "        mlflow.log_metric(\"loss\", (1 / (i + 0.1) + np.random.normal(0, 0.1)) , step=i)\n",
    "\n",
    "    # lets make some pretty graphs to store\n",
    "\n",
    "    graph = plt.figure()\n",
    "    plt.plot(range(100), [(i / 10) ** 2 for i in range(100)])\n",
    "    mlflow.log_figure(graph, \"figures/metric1.png\")\n",
    "\n",
    "    # alternative way to log a figure\n",
    "    graph = plt.figure()\n",
    "    plt.plot(range(100), [(i / 10) ** 3 for i in range(100)])\n",
    "    graph.savefig(\"metric2.png\")\n",
    "    mlflow.log_artifact(\"metric2.png\", artifact_path=\"figures\")\n",
    "    \n",
    "    # i need to add depenancies:\n",
    "    # lume-model @ git+https://github.com/slaclab/lume-model.git@2921e6583a6cfd49285833eb851b361aacf65b4c\n",
    "    \n",
    "    \n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model_files\",\n",
    "        python_model=model,\n",
    "        signature=infer_signature(input_sample, model.predict(None, input = input_sample)),\n",
    "        input_example=input_sample,\n",
    "        code_path=[\"model_definition.py\"],\n",
    "        extra_pip_requirements=[\"paho-mqtt\"] # example dependancy\n",
    "        # registered_model_name=\"generic_model\", # this will automatically register the model and iterate the version\n",
    "     )\n",
    "\n",
    "    # if you wanna log the model without the wrapper\n",
    "    model.save_model()\n",
    "    mlflow.log_artifact(\n",
    "        f\"{model.model_name}.txt\", artifact_path=\"model_files_no_mlflow\"\n",
    "    )\n",
    "    mlflow.log_artifact(\n",
    "        f\"pv_mapping.yaml\", artifact_path=\"generic_model\"\n",
    "    )\n",
    "\n",
    "    # set some tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we ignore keras for now\n",
    "# # keras model\n",
    "\n",
    "# inputs = [keras.Input(name=\"input1\", shape=(1,)), keras.Input(name=\"input2\", shape=(1,))]\n",
    "# x = keras.layers.concatenate(inputs)\n",
    "# x1 = keras.layers.Dense(64, activation='relu')(x)\n",
    "# x2 = keras.layers.Dense(64, activation='relu')(x1)\n",
    "# outputs = keras.layers.Dense(1, name=\"output\")(x2)\n",
    "# model_keras = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# mlflow.keras.autolog(log_models=False)\n",
    "# with mlflow.start_run() as run:\n",
    "#     model_keras.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#     model_keras.fit([data_X[:, 0], data_X[:, 1]], data_y, epochs=10)\n",
    "#     mlflow.set_tag(\"tag1\", \"tag_value1\")\n",
    "#     signature = infer_signature(data_X, model_keras.predict([data_X[:, 0], data_X[:, 1]]))\n",
    "#     model_info_keras = mlflow.keras.log_model(model_keras, \"keras_model\", signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lume_model.models import TorchModel, TorchModule\n",
    "\n",
    "\n",
    "# torch model \n",
    "base_torch = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "# change initialisation\n",
    "for layer in base_torch:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        torch.nn.init.zeros_(layer.bias)\n",
    "        layer.weight.data.fill_(0.05)\n",
    "        layer.bias.data.fill_(0.05)\n",
    "        \n",
    "\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(base_torch.parameters(), lr=0.01)\n",
    "    # log params\n",
    "    mlflow.log_param(\"lr\", 0.01)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"loss\", \"MSELoss\")\n",
    "    \n",
    "    for t in range(20):\n",
    "\n",
    "        X = torch.tensor(data_X, dtype=torch.float32)\n",
    "        y = torch.tensor(data_y, dtype=torch.float32)\n",
    "        y_pred = base_torch(X).flatten()\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        mlflow.log_metric(\"loss\", np.mean(loss.detach().numpy()), step=t)\n",
    "        print(t, np.mean(loss.detach().numpy()))\n",
    "    \n",
    "    mlflow.set_tag(\"tag1\", \"tag_value1\")\n",
    "    \n",
    "    lume_torch = TorchModel(model = base_torch, input_variables = input_variables, output_variables = output_variables)\n",
    "\n",
    "    model_torch = TorchModule(model = lume_torch)\n",
    "    \n",
    "    mlflow.log_artifact(\n",
    "        f\"torch/pv_mapping.yaml\", artifact_path=\"torch_model\"\n",
    "    )\n",
    "        \n",
    "        \n",
    "    signature = infer_signature(data_X, model_torch(torch.tensor(data_X, dtype=torch.float32)).detach().numpy()) # optional but useful\n",
    "\n",
    "    model_info_torch = mlflow.pytorch.log_model(model_torch, \"torch_model\", signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifc model uris\n",
    "print(model_info.model_uri)\n",
    "# print(model_info_keras.model_uri)\n",
    "print(model_info_torch.model_uri)\n",
    "\n",
    "# lets register the models if they are not already\n",
    "client = MlflowClient()\n",
    "try:\n",
    "    client.create_registered_model(\"generic_model\")\n",
    "    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "# try:\n",
    "#     client.create_registered_model(\"keras_model\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "try:\n",
    "    client.create_registered_model(\"torch_model\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# create new model versions\n",
    "\n",
    "# str(dict_model)\n",
    "result_generic = client.create_model_version(\n",
    "    name=\"generic_model\",\n",
    "    source=model_info.model_uri,\n",
    "    run_id=model_info.run_id,\n",
    "    tags={\"tests\": \"pending\", \"framework\": \"pyfunc\"},\n",
    ")\n",
    "\n",
    "\n",
    "# result_keras = client.create_model_version(\n",
    "#     name=\"keras_model\",\n",
    "#     source=model_info_keras.model_uri,\n",
    "#     run_id=model_info_keras.run_id,\n",
    "# )\n",
    "\n",
    "\n",
    "result_torch = client.create_model_version(\n",
    "    name=\"torch_model\",\n",
    "    source=model_info_torch.model_uri,\n",
    "    run_id=model_info_torch.run_id,\n",
    ")\n",
    "\n",
    "client.set_registered_model_alias(\"generic_model\", \"challenger\", result_generic.version)\n",
    "\n",
    "\n",
    "client.set_registered_model_tag(\"generic_model\", \"deployment_publish\", \"false\")\n",
    "client.set_registered_model_tag(\"generic_model\", \"deployment_type\", \"continuous\")\n",
    "\n",
    "\n",
    "# do some tests\n",
    "model_ver_champ = client.get_model_version_by_alias(\"generic_model\", \"challenger\").version\n",
    "\n",
    "# 99% of the time it will pass\n",
    "if np.random.uniform() < 0.99:\n",
    "    client.set_model_version_tag(\"generic_model\", f\"{model_ver_champ}\", \"tests\", \"passed\")\n",
    "    # set the champion model\n",
    "    client.set_registered_model_alias(\"generic_model\", \"champion\", result_generic.version)\n",
    "else:\n",
    "    client.set_model_version_tag(\"generic_model\", f\"{model_ver_champ}\", \"tests\", \"failed\")\n",
    "    print(\"model failed\")\n",
    "\n",
    "# client.set_registered_model_alias(\"keras_model\", \"champion\", result_keras.version)\n",
    "# # client.set_registered_model_tag(\"keras_model\", \"lattice_component\", \"DEF\")\n",
    "# # client.set_registered_model_tag(\"keras_model\", \"lume_service\", \"false\")\n",
    "# # client.set_registered_model_tag(\"keras_model\", \"lume_service_url\", \"\")\n",
    "# # client.set_registered_model_tag(\"keras_model\", \"deployment_type\", \"continuous\")\n",
    "# # client.set_registered_model_tag(\"keras_model\", \"multi_model_service\", \"false\")\n",
    "# # client.set_registered_model_tag(\"keras_model\", \"retrain\", \"false\")\n",
    "# # client.set_registered_model_tag(\"keras_model\", \"retrain_endpoint\", \"\")\n",
    "\n",
    "client.set_registered_model_alias(\"torch_model\", \"champion\", result_torch.version)\n",
    "client.set_registered_model_tag(\"torch_model\", \"deployment_type\", \"continuous\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model ids from registered models\n",
    "genric_model  = mlflow.pyfunc.load_model(f\"models:/generic_model@champion\")\n",
    "# keras_model = mlflow.keras.load_model(f\"models:/keras_model@champion\")\n",
    "torch_model = mlflow.pytorch.load_model(f\"models:/torch_model@champion\")\n",
    "\n",
    "# # get model info\n",
    "print(\"Generic model\")\n",
    "print(genric_model, type(genric_model))\n",
    "# print(\"Keras model\")\n",
    "# print(keras_model, type(keras_model)  \n",
    "print(\"Torch model\")\n",
    "print(torch_model, type(torch_model))\n",
    "\n",
    "# new_data = np.random.uniform(-1, 1, (20, 2))\n",
    "# print(new_data)\n",
    "\n",
    "# lets run some predictions\n",
    "\n",
    "# print(\"Generic model\")\n",
    "# print(genric_model.predict(new_data))\n",
    "# print(\"Keras model\")\n",
    "# print(keras_model.predict([new_data[:, 0], new_data[:, 1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(genric_model.metadata.get_input_schema())\n",
    "\n",
    "print(genric_model.metadata.get_output_schema())\n",
    "\n",
    "# results:\n",
    "# [Tensor('float64', (-1, 2))]\n",
    "# [Tensor('float64', (-1,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can create a wrapper for these models into lume-models\n",
    "# from lume_model.base import LUMEBaseModel\n",
    "# from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n",
    "# import torch\n",
    "# import keras\n",
    "# from test import MyModel\n",
    "\n",
    "# input_variables = [\n",
    "#     ScalarInputVariable(name=\"input1\", default=0.1, value_range=[0.0, 1.0]),\n",
    "#     ScalarInputVariable(name=\"input2\", default=0.2, value_range=[0.0, 1.0]),\n",
    "# ]\n",
    "# output_variables = [\n",
    "#     ScalarOutputVariable(name=\"output1\"),\n",
    "# ]\n",
    "\n",
    "# class ExampleModel(LUMEBaseModel):\n",
    "#     def evaluate(self, input_dict: dict[str, Any]) -> dict[str, Any]:\n",
    "#         return {\"outputs\": [0]}\n",
    "\n",
    "\n",
    "# m = ExampleModel(input_variables=input_variables, output_variables=output_variables)\n",
    "\n",
    "\n",
    "\n",
    "# with open(\"model.pkl\", \"wb\") as f:\n",
    "#     cp.dump(m , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# from lume_model.models import TorchModel, TorchModule\n",
    "# from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n",
    "\n",
    "# # exemplary model definition\n",
    "# base_model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(2, 1),\n",
    "# )\n",
    "\n",
    "# # variable specification\n",
    "# input_variables = [\n",
    "#     ScalarInputVariable(name=\"input1\", default=0.1, value_range=[0.0, 1.0]),\n",
    "#     ScalarInputVariable(name=\"input2\", default=0.2, value_range=[0.0, 1.0]),\n",
    "# ]\n",
    "# output_variables = [\n",
    "#     ScalarOutputVariable(name=\"output\"),\n",
    "# ]\n",
    "\n",
    "# # creation of TorchModel\n",
    "# example_model = TorchModel(\n",
    "#     model=base_model,\n",
    "#     input_variables=input_variables,\n",
    "#     output_variables=output_variables,\n",
    "# )\n",
    "\n",
    "# cp.dump(example_model, open(\"torch_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# import numpy as np\n",
    "\n",
    "# from lume_model.models import KerasModel\n",
    "# from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n",
    "\n",
    "# # exemplary model definition\n",
    "# inputs = [keras.Input(name=\"input1\", shape=(1,)), keras.Input(name=\"input2\", shape=(1,))]\n",
    "# outputs = keras.layers.Dense(1, activation=keras.activations.relu)(keras.layers.concatenate(inputs))\n",
    "# base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# # variable specification\n",
    "# input_variables = [\n",
    "#     ScalarInputVariable(name=inputs[0].name, default=0.1, value_range=[0.0, 1.0]),\n",
    "#     ScalarInputVariable(name=inputs[1].name, default=0.2, value_range=[0.0, 1.0]),\n",
    "# ]\n",
    "# output_variables = [\n",
    "#     ScalarOutputVariable(name=\"output\"),\n",
    "# ]\n",
    "\n",
    "# # creation of KerasModel\n",
    "# example_model = KerasModel(\n",
    "#     model=base_model,\n",
    "#     input_variables=input_variables,\n",
    "#     output_variables=output_variables,\n",
    "# )\n",
    "# cp.dump( example_model, open(\"keras_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from test import ExampleModel\n",
    "# from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n",
    "# import cloudpickle as cp\n",
    "\n",
    "# input_variables = [\n",
    "#     ScalarInputVariable(name=\"input1\", default=0.1, value_range=[0.0, 1.0]),\n",
    "#     ScalarInputVariable(name=\"input2\", default=0.2, value_range=[0.0, 1.0]),\n",
    "# ]\n",
    "# output_variables = [\n",
    "#     ScalarOutputVariable(name=\"output1\"),\n",
    "#     ScalarOutputVariable(name=\"output2\"),\n",
    "# ]\n",
    "\n",
    "# m = ExampleModel(input_variables=input_variables, output_variables=output_variables)\n",
    "\n",
    "# cp.dump(m, open(\"example_model.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
